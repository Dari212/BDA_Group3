{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacker News Sentiment Analysis with Kafka + PySpark + MongoDB\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook demonstrates a complete big data pipeline:\n",
    "\n",
    "1. **Data Collection**: Stream Hacker News posts + comments via API\n",
    "2. **Kafka Streaming**: Push posts to Kafka topic. We will be using cloud kafka with confluence, as otherwise this would not be possible.\n",
    "3. **PySpark Processing**: \n",
    "   - Subscribe to Kafka stream\n",
    "   - Fetch comments for each post\n",
    "   - Calculate sentiment scores on comments\n",
    "   - Aggregate overall post sentiment\n",
    "4. **MongoDB Integration**: Store enriched results with sentiment analysis\n",
    "5. **Analytics**: Query and visualize sentiment trends\n",
    "\n",
    "### Use Case\n",
    "**Predict community reaction to HN posts** by analyzing comment sentiment in real-time.\n",
    "\n",
    "### What This Notebook Demonstrates:\n",
    "\n",
    "1. **Kafka Streaming** - Real-time data ingestion\n",
    "2. **PySpark DataFrames** - Distributed data processing\n",
    "3. **Spark SQL** - SQL queries on streaming data\n",
    "4. **MongoDB Integration** - NoSQL persistence\n",
    "5. **NLP/Sentiment Analysis** - TextBlob for comment analysis\n",
    "6. **Windowed Aggregations** - Time-based analytics\n",
    "7. **UDFs** - Custom sentiment calculation functions\n",
    "8. **Pipeline Architecture** - Complete streaming + batch analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Confluent Cloud Configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate venv\n",
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pyspark==3.5.0) (0.10.9.7)\n",
      "Requirement already satisfied: kafka-python in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: textblob in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: pymongo in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.15.5)\n",
      "Requirement already satisfied: nltk>=3.9 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pymongo) (2.8.0)\n",
      "Requirement already satisfied: click in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk>=3.9->textblob) (8.3.1)\n",
      "Requirement already satisfied: joblib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk>=3.9->textblob) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk>=3.9->textblob) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from nltk>=3.9->textblob) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# We need to downgrade pyspark to this version\n",
    "!pip install pyspark==3.5.0\n",
    "!pip install kafka-python textblob pymongo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens==3.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: certifi==2025.11.12 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 2)) (2025.11.12)\n",
      "Requirement already satisfied: charset-normalizer==3.4.4 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 3)) (3.4.4)\n",
      "Requirement already satisfied: click==8.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 4)) (8.3.1)\n",
      "Requirement already satisfied: comm==0.2.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 5)) (0.2.3)\n",
      "Requirement already satisfied: debugpy==1.8.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 6)) (1.8.17)\n",
      "Requirement already satisfied: decorator==5.2.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 7)) (5.2.1)\n",
      "Requirement already satisfied: dnspython==2.8.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 8)) (2.8.0)\n",
      "Requirement already satisfied: executing==2.2.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 9)) (2.2.1)\n",
      "Requirement already satisfied: idna==3.11 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 10)) (3.11)\n",
      "Requirement already satisfied: ipykernel==7.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 11)) (7.1.0)\n",
      "Requirement already satisfied: ipython==9.8.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 12)) (9.8.0)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 13)) (1.1.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 14)) (0.19.2)\n",
      "Requirement already satisfied: joblib==1.5.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 15)) (1.5.2)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 16)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.9.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 17)) (5.9.1)\n",
      "Requirement already satisfied: kafka-python==2.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 18)) (2.3.0)\n",
      "Requirement already satisfied: matplotlib-inline==0.2.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 19)) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 20)) (1.6.0)\n",
      "Requirement already satisfied: nltk==3.9.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 21)) (3.9.2)\n",
      "Requirement already satisfied: packaging==25.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 22)) (25.0)\n",
      "Requirement already satisfied: parso==0.8.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 23)) (0.8.5)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 24)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs==4.5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 25)) (4.5.1)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.52 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 26)) (3.0.52)\n",
      "Requirement already satisfied: psutil==7.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 27)) (7.1.3)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 28)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 29)) (0.2.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 30)) (0.10.9.7)\n",
      "Requirement already satisfied: Pygments==2.19.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 31)) (2.19.2)\n",
      "Requirement already satisfied: pymongo==4.15.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 32)) (4.15.5)\n",
      "Requirement already satisfied: pyspark==3.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 33)) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 34)) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq==27.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 35)) (27.1.0)\n",
      "Requirement already satisfied: regex==2025.11.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 36)) (2025.11.3)\n",
      "Requirement already satisfied: requests==2.32.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 37)) (2.32.5)\n",
      "Requirement already satisfied: six==1.17.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 38)) (1.17.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 39)) (0.6.3)\n",
      "Requirement already satisfied: textblob==0.19.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 40)) (0.19.0)\n",
      "Requirement already satisfied: tornado==6.5.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 41)) (6.5.2)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 42)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 43)) (5.14.3)\n",
      "Requirement already satisfied: urllib3==2.6.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 44)) (2.6.0)\n",
      "Requirement already satisfied: wcwidth==0.2.14 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from -r requirements_streming.txt (line 45)) (0.2.14)\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -r requirements_streming.txt\n",
    "\n",
    "# Download TextBlob corpora \n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Detected Linux environment (Lightning AI)\n",
      "================================================================================\n",
      "CONFLUENT CLOUD CONFIGURATION\n",
      "================================================================================\n",
      "Python: 3.12.11\n",
      "Java: java-17-openjdk-amd64\n",
      "MongoDB: hackernews_analytics.post_sentiment\n",
      "Kafka Bootstrap: pkc-921jm.us-east-2.aws.confluent.cloud:9092\n",
      "Kafka Topic: hackernews_posts\n",
      "Kafka Auth: ✓ Enabled (SASL_SSL)\n",
      "================================================================================\n",
      "\n",
      "✅ Confluent Cloud configured! Ready to connect.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# MongoDB Configuration. The Passwords are hidden for security reasons, as I was unable to set up a .env file for demonstration purposes\n",
    "MONGO_URI = \"mongodb+srv://carolina:PASSWORD@cluster0.32afvrm.mongodb.net/?appName=Cluster0\"\n",
    "MONGO_DATABASE = \"hackernews_analytics\"\n",
    "MONGO_COLLECTION = \"post_sentiment\"\n",
    "\n",
    "# Confluent Cloud Kafka Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"pkc-921jm.us-east-2.aws.confluent.cloud:9092\"  \n",
    "KAFKA_TOPIC = \"hackernews_posts\"\n",
    "KAFKA_SASL_USERNAME = \"5SJO32G3QUXNEE6P\" \n",
    "KAFKA_SASL_PASSWORD = PASSWORD\n",
    "\n",
    "# Hacker News API\n",
    "HN_API_BASE = \"https://hacker-news.firebaseio.com/v0\"\n",
    "\n",
    "\n",
    "# Configure Java for PySpark\n",
    "JAVA_HOME_LINUX = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "JAVA_HOME_MACOS = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "\n",
    "if os.path.exists(JAVA_HOME_LINUX):\n",
    "    JAVA_HOME = JAVA_HOME_LINUX\n",
    "    print(\"Detected Linux environment (Lightning AI)\")\n",
    "elif os.path.exists(JAVA_HOME_MACOS):\n",
    "    JAVA_HOME = JAVA_HOME_MACOS\n",
    "    print(\"✓ Detected macOS environment (local)\")\n",
    "else:\n",
    "    print(\"Java not found. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"apt-get\", \"update\"], check=False, capture_output=True)\n",
    "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"openjdk-17-jdk\"], check=False, capture_output=True)\n",
    "    JAVA_HOME = JAVA_HOME_LINUX\n",
    "    print(\"Java 17 installed\")\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
    "os.environ[\"PATH\"] = f\"{JAVA_HOME}/bin:{os.environ.get('PATH', '')}\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"MONGO_URI\"] = MONGO_URI\n",
    "os.environ[\"MONGO_DATABASE\"] = MONGO_DATABASE\n",
    "os.environ[\"MONGO_COLLECTION\"] = MONGO_COLLECTION\n",
    "os.environ[\"KAFKA_BOOTSTRAP_SERVERS\"] = KAFKA_BOOTSTRAP_SERVERS\n",
    "os.environ[\"KAFKA_TOPIC\"] = KAFKA_TOPIC\n",
    "if KAFKA_SASL_USERNAME:\n",
    "    os.environ[\"KAFKA_SASL_USERNAME\"] = KAFKA_SASL_USERNAME\n",
    "if KAFKA_SASL_PASSWORD:\n",
    "    os.environ[\"KAFKA_SASL_PASSWORD\"] = KAFKA_SASL_PASSWORD\n",
    "\n",
    "USE_KAFKA_AUTH = bool(KAFKA_SASL_USERNAME and KAFKA_SASL_PASSWORD)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFLUENT CLOUD CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"Java: {JAVA_HOME.split('/')[-1]}\")\n",
    "print(f\"MongoDB: {MONGO_DATABASE}.{MONGO_COLLECTION}\")\n",
    "print(f\"Kafka Bootstrap: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
    "print(f\"Kafka Topic: {KAFKA_TOPIC}\")\n",
    "print(f\"Kafka Auth: {'✓ Enabled (SASL_SSL)' if USE_KAFKA_AUTH else '✗ NOT CONFIGURED - Set API Key and Secret!'}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Validation\n",
    "if not USE_KAFKA_AUTH:\n",
    "    print(\"\\n  WARNING: Confluent Cloud credentials not set!\")\n",
    "    print(\"\")\n",
    "    print(\"To set up Confluent Cloud:\")\n",
    "    print(\"1. Go to https://confluent.cloud\")\n",
    "    print(\"2. Sign up (free, no credit card)\")\n",
    "    print(\"3. Create 'Basic' cluster\")\n",
    "    print(\"4. Create topic: hackernews_posts\")\n",
    "    print(\"5. Generate API keys (Cluster → API Keys)\")\n",
    "    print(\"6. Get Bootstrap server (Cluster Settings)\")\n",
    "    print(\"7. Update Cell 0 above with your credentials\")\n",
    "    print(\"\")\n",
    "    print(\"See KAFKA_CLOUD_SETUP.md for detailed instructions\")\n",
    "    print(\"\")\n",
    "else:\n",
    "    print(\"\\n ✓ Confluent Cloud configured! Ready to connect.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StringType, IntegerType, StructField, \n",
    "    TimestampType, DoubleType, ArrayType\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, col, avg, count, lit\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from kafka import KafkaProducer\n",
    "from textblob import TextBlob\n",
    "import pymongo\n",
    "\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "# Configuration\n",
    "KAFKA_TOPIC = \"hackernews_posts\"\n",
    "HN_API_BASE = \"https://hacker-news.firebaseio.com/v0\"\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MongoDB Setup\n",
    "\n",
    "Configure MongoDB connection for storing sentiment analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB configuration set\n"
     ]
    }
   ],
   "source": [
    "# MongoDB Configuration\n",
    "MONGO_URI = \"mongodb+srv://carolina:carolina@cluster0.32afvrm.mongodb.net/?appName=Cluster0\"\n",
    "MONGO_DATABASE = \"hackernews_analytics\"\n",
    "MONGO_COLLECTION = \"post_sentiment\"\n",
    "\n",
    "print(\"MongoDB configuration set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully connected to MongoDB\n",
      "✓ Database: hackernews_analytics\n",
      "✓ Collection: post_sentiment\n"
     ]
    }
   ],
   "source": [
    "# Test MongoDB connection\n",
    "try:\n",
    "    client = pymongo.MongoClient(MONGO_URI)\n",
    "    # Test connection\n",
    "    client.admin.command('ping')\n",
    "    print(\"✓ Successfully connected to MongoDB\")\n",
    "    \n",
    "    # Create database and collection\n",
    "    db = client[MONGO_DATABASE]\n",
    "    collection = db[MONGO_COLLECTION]\n",
    "    \n",
    "    print(f\"✓ Database: {MONGO_DATABASE}\")\n",
    "    print(f\"✓ Collection: {MONGO_COLLECTION}\")\n",
    "    \n",
    "    client.close()\n",
    "except Exception as e:\n",
    "    print(f\"✗ MongoDB connection failed: {e}\")\n",
    "    print(\"Make sure to update your credentials above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Spark Session with Kafka + MongoDB Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/system/conda/miniconda3/envs/cloudspace/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/zeus/.ivy2/cache\n",
      "The jars for the packages stored in: /home/zeus/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-430aba07-b642-4355-968f-4e7c14086bb2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      ":: resolution report :: resolve 1955ms :: artifacts dl 55ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   1   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-430aba07-b642-4355-968f-4e7c14086bb2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/14ms)\n",
      "25/12/10 23:05:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/10 23:05:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark 3.5.0 initialized with Kafka + MongoDB connectors\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark with both Kafka and MongoDB connectors\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"HackerNews_Sentiment_MongoDB\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages',\n",
    "            'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,'\n",
    "            'org.mongodb.spark:mongo-spark-connector_2.12:10.4.0') \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", MONGO_URI) \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", MONGO_URI) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"✓ Spark {spark.version} initialized with Kafka + MongoDB connectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hacker News API Functions\n",
    "\n",
    "Functions to fetch posts and comments from HN API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HN API functions defined\n"
     ]
    }
   ],
   "source": [
    "def fetch_item(item_id):\n",
    "    \"\"\"\n",
    "    Fetch a single item (post or comment) from HN API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{HN_API_BASE}/item/{item_id}.json\")\n",
    "        return response.json() if response.status_code == 200 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_comments(comment_ids, max_comments=20):\n",
    "    \"\"\"\n",
    "    Fetch multiple comments by their IDs.\n",
    "    Returns list of comment texts.\n",
    "    \"\"\"\n",
    "    if not comment_ids:\n",
    "        return []\n",
    "    \n",
    "    comments = []\n",
    "    for cid in comment_ids[:max_comments]:  # Limit to avoid rate limits\n",
    "        comment = fetch_item(cid)\n",
    "        if comment and comment.get('text'):\n",
    "            comments.append(comment['text'])\n",
    "        time.sleep(0.05)  # Rate limiting\n",
    "    \n",
    "    return comments\n",
    "\n",
    "\n",
    "def calculate_comment_sentiment(comments):\n",
    "    \"\"\"\n",
    "    Calculate average sentiment from list of comments.\n",
    "    Returns: (avg_sentiment, sentiment_label, comment_count)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if not comments:\n",
    "        return 0.0, \"neutral\", 0\n",
    "    \n",
    "    sentiments = []\n",
    "    for comment_text in comments:\n",
    "        try:\n",
    "            # Clean HTML entities and tags from HN comments\n",
    "            cleaned = unescape(comment_text)  # Convert &lt; to <, etc.\n",
    "            cleaned = re.sub('<[^<]+?>', '', cleaned)  # Remove HTML tags\n",
    "            cleaned = cleaned.strip()\n",
    "            \n",
    "            if not cleaned:\n",
    "                continue\n",
    "            \n",
    "            blob = TextBlob(cleaned)\n",
    "            sentiments.append(blob.sentiment.polarity)\n",
    "        except Exception as e:\n",
    "            # Print error for debugging\n",
    "            print(f\"Warning: Failed to analyze comment: {str(e)[:50]}\")\n",
    "            continue\n",
    "    \n",
    "    if not sentiments:\n",
    "        return 0.0, \"neutral\", 0\n",
    "    \n",
    "    avg_sentiment = sum(sentiments) / len(sentiments)\n",
    "    \n",
    "    # Classify sentiment\n",
    "    if avg_sentiment > 0.1:\n",
    "        label = \"positive\"\n",
    "    elif avg_sentiment < -0.1:\n",
    "        label = \"negative\"\n",
    "    else:\n",
    "        label = \"neutral\"\n",
    "    \n",
    "    return avg_sentiment, label, len(sentiments)\n",
    "\n",
    "\n",
    "print(\"✓ HN API functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kafka Producer - Stream HN Posts with Comment Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Kafka producer function defined\n"
     ]
    }
   ],
   "source": [
    "def hn_sentiment_producer(num_posts=50, delay=3):\n",
    "    \"\"\"\n",
    "    Fetch HN posts, analyze comment sentiment, and stream to Kafka.\n",
    "    \n",
    "    For each post:\n",
    "    1. Fetch post metadata\n",
    "    2. Fetch top comments\n",
    "    3. Calculate sentiment from comments\n",
    "    4. Stream enriched data to Kafka\n",
    "    \"\"\"\n",
    "    # Configure Kafka producer\n",
    "    producer_config = {\n",
    "        'bootstrap_servers': KAFKA_BOOTSTRAP_SERVERS,\n",
    "        'value_serializer': lambda v: json.dumps(v).encode('utf-8')\n",
    "    }\n",
    "    \n",
    "    # Add SASL authentication if configured\n",
    "    if USE_KAFKA_AUTH:\n",
    "        producer_config.update({\n",
    "            'security_protocol': 'SASL_SSL',\n",
    "            'sasl_mechanism': 'PLAIN',\n",
    "            'sasl_plain_username': KAFKA_SASL_USERNAME,\n",
    "            'sasl_plain_password': KAFKA_SASL_PASSWORD\n",
    "        })\n",
    "    \n",
    "    producer = KafkaProducer(**producer_config)\n",
    "    \n",
    "    print(\"Fetching latest HN stories...\")\n",
    "    story_ids = requests.get(f\"{HN_API_BASE}/topstories.json\").json()[:num_posts]\n",
    "    \n",
    "    print(f\"Processing {len(story_ids)} posts with sentiment analysis...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    successful = 0\n",
    "    \n",
    "    for idx, story_id in enumerate(story_ids, 1):\n",
    "        try:\n",
    "            # Fetch post\n",
    "            story = fetch_item(story_id)\n",
    "            \n",
    "            if not story or story.get('type') != 'story':\n",
    "                continue\n",
    "            \n",
    "            # Extract post metadata\n",
    "            title = story.get('title', '')\n",
    "            url = story.get('url', '')\n",
    "            score = story.get('score', 0)\n",
    "            by = story.get('by', 'unknown')\n",
    "            comment_ids = story.get('kids', [])\n",
    "            comment_count = len(comment_ids)\n",
    "            \n",
    "            # Fetch and analyze comments\n",
    "            print(f\"[{idx}/{len(story_ids)}] Analyzing: {title[:50]}...\")\n",
    "            comments = fetch_comments(comment_ids, max_comments=20)\n",
    "            \n",
    "            avg_sentiment, sentiment_label, analyzed_comments = calculate_comment_sentiment(comments)\n",
    "            \n",
    "            # Create enriched message\n",
    "            message = {\n",
    "                \"post_id\": story_id,\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"score\": score,\n",
    "                \"author\": by,\n",
    "                \"total_comments\": comment_count,\n",
    "                \"analyzed_comments\": analyzed_comments,\n",
    "                \"avg_sentiment_score\": round(avg_sentiment, 4),\n",
    "                \"sentiment_label\": sentiment_label,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"sample_comments\": comments[:3]  # Store sample for reference\n",
    "            }\n",
    "            \n",
    "            # Send to Kafka\n",
    "            producer.send(KAFKA_TOPIC, message)\n",
    "            successful += 1\n",
    "            \n",
    "            print(f\"  → Score: {score} | Comments: {analyzed_comments}/{comment_count} | \"\n",
    "                  f\"Sentiment: {sentiment_label} ({avg_sentiment:.3f})\")\n",
    "            \n",
    "            if idx % 10 == 0:\n",
    "                print(f\"\\n Progress: {idx}/{len(story_ids)} ({successful} successful)\\n\")\n",
    "            \n",
    "            time.sleep(delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error processing story {story_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"\\n Finished! Streamed {successful} posts with sentiment analysis to Kafka\")\n",
    "\n",
    "\n",
    "print(\"✓ Kafka producer function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully connected to Kafka\n",
      "  Bootstrap Servers: pkc-921jm.us-east-2.aws.confluent.cloud:9092\n",
      "  Authentication: Enabled (SASL_SSL)\n"
     ]
    }
   ],
   "source": [
    "# Test Kafka connection\n",
    "try:\n",
    "    # Configure producer with authentication if needed\n",
    "    kafka_config = {\n",
    "        'bootstrap_servers': KAFKA_BOOTSTRAP_SERVERS,\n",
    "        'request_timeout_ms': 5000,\n",
    "        'api_version_auto_timeout_ms': 5000\n",
    "    }\n",
    "    \n",
    "    # Add SASL authentication if using cloud Kafka\n",
    "    if USE_KAFKA_AUTH:\n",
    "        kafka_config.update({\n",
    "            'security_protocol': 'SASL_SSL',\n",
    "            'sasl_mechanism': 'PLAIN',\n",
    "            'sasl_plain_username': KAFKA_SASL_USERNAME,\n",
    "            'sasl_plain_password': KAFKA_SASL_PASSWORD\n",
    "        })\n",
    "    \n",
    "    test_producer = KafkaProducer(**kafka_config)\n",
    "    test_producer.close()\n",
    "    print(\"✓ Successfully connected to Kafka\")\n",
    "    print(f\"  Bootstrap Servers: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
    "    print(f\"  Authentication: {'Enabled (SASL_SSL)' if USE_KAFKA_AUTH else 'Disabled'}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Could not connect to Kafka: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: PRODUCE DATA FIRST\n",
    "\n",
    "**Now we'll produce data to Kafka BEFORE setting up consumers.**\n",
    "\n",
    "This follows the logical flow:\n",
    "1. Setup  (MongoDB, Spark, API functions)\n",
    "2. **Produce data** - Stream HN posts with sentiment to Kafka\n",
    "3. ⏭Then consume - Set up PySpark to read from Kafka\n",
    "\n",
    "**What this does:**\n",
    "- Fetches 30 Hacker News posts from the API\n",
    "- Analyzes comment sentiment for each post\n",
    "- Streams enriched data to Kafka topic `hackernews_posts`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching latest HN stories...\n",
      "Processing 30 posts with sentiment analysis...\n",
      "--------------------------------------------------------------------------------\n",
      "[1/30] Analyzing: Size of Life...\n",
      "  → Score: 1138 | Comments: 20/79 | Sentiment: positive (0.270)\n",
      "[2/30] Analyzing: Australia begins enforcing world-first teen social...\n",
      "  → Score: 434 | Comments: 20/85 | Sentiment: neutral (0.033)\n",
      "[3/30] Analyzing: Getting a Gemini API key is an exercise in frustra...\n",
      "  → Score: 88 | Comments: 17/19 | Sentiment: positive (0.136)\n",
      "[4/30] Analyzing: Super Mario 64 for the PS1...\n",
      "  → Score: 123 | Comments: 9/9 | Sentiment: neutral (0.051)\n",
      "[5/30] Analyzing: Auto-grading decade-old Hacker News discussions wi...\n",
      "  → Score: 223 | Comments: 19/39 | Sentiment: positive (0.181)\n",
      "[6/30] Analyzing: When would you ever want bubblesort? (2023)...\n",
      "  → Score: 29 | Comments: 10/10 | Sentiment: neutral (0.006)\n",
      "[7/30] Analyzing: How Google Maps allocates survival across London's...\n",
      "  → Score: 61 | Comments: 7/7 | Sentiment: positive (0.230)\n",
      "[8/30] Analyzing: Terrain Diffusion: A Diffusion-Based Successor to ...\n",
      "  → Score: 76 | Comments: 6/6 | Sentiment: positive (0.162)\n",
      "[9/30] Analyzing: Rubio stages font coup: Times New Roman ousts Cali...\n",
      "  → Score: 76 | Comments: 20/60 | Sentiment: positive (0.139)\n",
      "[10/30] Analyzing: Scientists create ultra fast memory using light...\n",
      "  → Score: 48 | Comments: 6/6 | Sentiment: positive (0.206)\n",
      "\n",
      "✓ Progress: 10/30 (10 successful)\n",
      "\n",
      "[11/30] Analyzing: Qwen3-Omni-Flash-2025-12-01：a next-generation nati...\n",
      "  → Score: 166 | Comments: 14/15 | Sentiment: neutral (0.093)\n",
      "[12/30] Analyzing: Common Lisp, ASDF, and Quicklisp: packaging explai...\n",
      "  → Score: 13 | Comments: 0/0 | Sentiment: neutral (0.000)\n",
      "[13/30] Analyzing: Gundam is just the same as Jane Austen but happens...\n",
      "  → Score: 118 | Comments: 20/20 | Sentiment: positive (0.236)\n",
      "[14/30] Analyzing: The future of Terraform CDK...\n",
      "  → Score: 62 | Comments: 15/15 | Sentiment: positive (0.128)\n",
      "[15/30] Analyzing: Show HN: Automated license plate reader coverage i...\n",
      "  → Score: 80 | Comments: 12/13 | Sentiment: neutral (0.041)\n",
      "[16/30] Analyzing: Valve: HDMI Forum Continues to Block HDMI 2.1 for ...\n",
      "  → Score: 420 | Comments: 20/20 | Sentiment: neutral (0.074)\n",
      "[17/30] Analyzing: Launch HN: InspectMind (YC W24) – AI agent for rev...\n",
      "  → Score: 34 | Comments: 10/10 | Sentiment: neutral (0.053)\n",
      "[19/30] Analyzing: Is it a bubble?...\n",
      "  → Score: 103 | Comments: 19/25 | Sentiment: positive (0.130)\n",
      "[20/30] Analyzing: Apple Services Experiencing Outage...\n",
      "  → Score: 97 | Comments: 11/13 | Sentiment: neutral (-0.078)\n",
      "\n",
      "✓ Progress: 20/30 (19 successful)\n",
      "\n",
      "[21/30] Analyzing: Show HN: VoxCSS – A DOM based voxel engine...\n",
      "  → Score: 12 | Comments: 0/0 | Sentiment: neutral (0.000)\n",
      "[22/30] Analyzing: DeepSeek uses banned Nvidia chips for AI model, re...\n",
      "  → Score: 269 | Comments: 20/51 | Sentiment: neutral (0.044)\n",
      "[23/30] Analyzing: Largest EV manufacturer is coming to the Western m...\n",
      "  → Score: 23 | Comments: 7/8 | Sentiment: neutral (0.042)\n",
      "[24/30] Analyzing: Show HN: A 2-row, 16-key keyboard designed for sma...\n",
      "  → Score: 36 | Comments: 15/15 | Sentiment: neutral (0.083)\n",
      "[25/30] Analyzing: Should CSS be constraints?...\n",
      "  → Score: 6 | Comments: 13/13 | Sentiment: positive (0.186)\n",
      "[26/30] Analyzing: Typewriter Plotters (2022)...\n",
      "  → Score: 83 | Comments: 3/3 | Sentiment: neutral (0.084)\n",
      "[27/30] Analyzing: Factor 0.101 now available...\n",
      "  → Score: 95 | Comments: 7/7 | Sentiment: positive (0.235)\n",
      "[28/30] Analyzing: Golang's big miss on memory arenas...\n",
      "  → Score: 77 | Comments: 19/24 | Sentiment: positive (0.111)\n",
      "[29/30] Analyzing: EFF Launches Age Verification Hub as Resource Agai...\n",
      "  → Score: 68 | Comments: 6/6 | Sentiment: positive (0.191)\n",
      "[30/30] Analyzing: RoboCrop: Teaching robots how to pick tomatoes...\n",
      "  → Score: 56 | Comments: 6/7 | Sentiment: neutral (0.020)\n",
      "\n",
      "✓ Progress: 30/30 (29 successful)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ Finished! Streamed 29 posts with sentiment analysis to Kafka\n"
     ]
    }
   ],
   "source": [
    "# Start streaming posts with sentiment analysis\n",
    "# Uncomment and run when ready:\n",
    "\n",
    "hn_sentiment_producer(num_posts=30, delay=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PySpark Streaming - Read from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Schema defined\n"
     ]
    }
   ],
   "source": [
    "# Define schema for Kafka messages\n",
    "hn_sentiment_schema = StructType([\n",
    "    StructField(\"post_id\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"total_comments\", IntegerType(), True),\n",
    "    StructField(\"analyzed_comments\", IntegerType(), True),\n",
    "    StructField(\"avg_sentiment_score\", DoubleType(), True),\n",
    "    StructField(\"sentiment_label\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"sample_comments\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "print(\"✓ Schema defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to Kafka stream\n"
     ]
    }
   ],
   "source": [
    "# Configure Kafka options for Spark\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": KAFKA_BOOTSTRAP_SERVERS,\n",
    "    \"subscribe\": KAFKA_TOPIC,\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "\n",
    "# Add SASL configuration if using authenticated Kafka\n",
    "if USE_KAFKA_AUTH:\n",
    "    kafka_options.update({\n",
    "        \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "        \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "        \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{KAFKA_SASL_USERNAME}\" password=\"{KAFKA_SASL_PASSWORD}\";'\n",
    "    })\n",
    "\n",
    "# Subscribe to Kafka topic\n",
    "streaming_df = spark.readStream.format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .load()\n",
    "\n",
    "print(\"✓ Connected to Kafka stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Parsing and enrichment configured\n"
     ]
    }
   ],
   "source": [
    "# Parse JSON from Kafka\n",
    "parsed_df = streaming_df.select(\n",
    "    F.from_json(F.col(\"value\").cast(\"string\"), hn_sentiment_schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# Convert timestamp\n",
    "parsed_df = parsed_df.withColumn(\n",
    "    \"timestamp\",\n",
    "    F.to_timestamp(F.col(\"timestamp\"))\n",
    ")\n",
    "\n",
    "# Add derived columns\n",
    "enriched_df = parsed_df \\\n",
    "    .withColumn(\"engagement_ratio\", \n",
    "                F.col(\"total_comments\") / (F.col(\"score\") + 1)) \\\n",
    "    .withColumn(\"is_controversial\", \n",
    "                (F.col(\"total_comments\") > 50) & (F.col(\"avg_sentiment_score\") < 0)) \\\n",
    "    .withColumn(\"hour_of_day\", F.hour(\"timestamp\"))\n",
    "\n",
    "print(\"✓ Parsing and enrichment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write Stream to Console (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/10 23:07:39 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-07563b93-cf14-41e0-a965-a58b3eaa6142. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/12/10 23:07:39 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Console query started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/10 23:07:40 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "# Console output for debugging\n",
    "console_query = enriched_df \\\n",
    "    .select(\n",
    "        \"title\",\n",
    "        \"sentiment_label\",\n",
    "        \"avg_sentiment_score\",\n",
    "        \"score\",\n",
    "        \"total_comments\",\n",
    "        \"is_controversial\"\n",
    "    ) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"✓ Console query started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop console query when done viewing\n",
    "console_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write Stream to MongoDB\n",
    "\n",
    "Store sentiment analysis results in MongoDB for persistence and historical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MongoDB write function defined\n"
     ]
    }
   ],
   "source": [
    "def write_to_mongodb(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Write each micro-batch to MongoDB.\n",
    "    This function is called for each batch of streaming data.\n",
    "    \"\"\"\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Write to MongoDB using the connector\n",
    "        batch_df.write \\\n",
    "            .format(\"mongodb\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"database\", MONGO_DATABASE) \\\n",
    "            .option(\"collection\", MONGO_COLLECTION) \\\n",
    "            .save()\n",
    "        \n",
    "        count = batch_df.count()\n",
    "        print(f\" Batch {batch_id}: Wrote {count} records to MongoDB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error writing batch {batch_id} to MongoDB: {e}\")\n",
    "\n",
    "\n",
    "print(\"✓ MongoDB write function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MongoDB streaming query started\n",
      "Data will be written to MongoDB every 10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/10 23:07:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/10 23:07:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/12/10 23:08:08 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 10983 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Batch 1: Wrote 29 records to MongoDB\n"
     ]
    }
   ],
   "source": [
    "# Start streaming to MongoDB\n",
    "mongodb_query = enriched_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(write_to_mongodb) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoints/hn_mongodb\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"✓ MongoDB streaming query started\")\n",
    "print(\"Data will be written to MongoDB every 10 seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Memory Tables for Real-time SQL Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Memory table 'hn_sentiment_live' created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/10 23:09:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9819ddad-7d49-460a-9ee1-7ba6e7191183. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/12/10 23:09:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/10 23:09:13 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write to memory table for SQL queries\n",
    "memory_query = enriched_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"hn_sentiment_live\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"✓ Memory table 'hn_sentiment_live' created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+---------+-----------+------------+\n",
      "|sentiment_label|count|avg_score|avg_upvotes|avg_comments|\n",
      "+---------------+-----+---------+-----------+------------+\n",
      "|        neutral|   33|     0.04|      116.4|        16.4|\n",
      "|       positive|   30|    0.189|      156.3|        21.5|\n",
      "+---------------+-----+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query sentiment distribution\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        sentiment_label,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(AVG(avg_sentiment_score), 3) as avg_score,\n",
    "        ROUND(AVG(score), 1) as avg_upvotes,\n",
    "        ROUND(AVG(total_comments), 1) as avg_comments\n",
    "    FROM hn_sentiment_live\n",
    "    GROUP BY sentiment_label\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-------------------+-----+--------------+\n",
      "|title|sentiment_label|avg_sentiment_score|score|total_comments|\n",
      "+-----+---------------+-------------------+-----+--------------+\n",
      "+-----+---------------+-------------------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find controversial posts (negative sentiment + high engagement)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        title,\n",
    "        sentiment_label,\n",
    "        avg_sentiment_score,\n",
    "        score,\n",
    "        total_comments\n",
    "    FROM hn_sentiment_live\n",
    "    WHERE is_controversial = true\n",
    "    ORDER BY total_comments DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=60)\n",
    "\n",
    "# There were no generally negative sentiment posts, oddly enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+-------------------+-----+--------------+\n",
      "|                                                       title|avg_sentiment_score|score|total_comments|\n",
      "+------------------------------------------------------------+-------------------+-----+--------------+\n",
      "|I got an Nvidia GH200 server for €7.5k on Reddit and conv...|             0.3027|  132|            17|\n",
      "|                                                Size of Life|             0.2838| 1101|            77|\n",
      "|                                                Size of Life|             0.2704| 1138|            79|\n",
      "|Gundam is just the same as Jane Austen but happens to inc...|             0.2484|  114|            20|\n",
      "|Gundam is just the same as Jane Austen but happens to inc...|              0.236|  118|            20|\n",
      "|                                  Factor 0.101 now available|             0.2345|   95|             7|\n",
      "|                                  Factor 0.101 now available|             0.2345|   91|             7|\n",
      "|How Google Maps allocates survival across London's restau...|               0.23|   61|             7|\n",
      "|How Google Maps allocates survival across London's restau...|               0.23|   55|             7|\n",
      "|How Google Maps allocates survival across London's restau...|               0.23|   55|             7|\n",
      "+------------------------------------------------------------+-------------------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top positive posts\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        title,\n",
    "        avg_sentiment_score,\n",
    "        score,\n",
    "        total_comments\n",
    "    FROM hn_sentiment_live\n",
    "    WHERE sentiment_label = 'positive'\n",
    "    ORDER BY avg_sentiment_score DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Windowed Sentiment Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowed aggregations - sentiment trends over time\n",
    "windowed_sentiment = enriched_df \\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(\n",
    "        F.window(\"timestamp\", \"10 minutes\", \"5 minutes\"),\n",
    "        \"sentiment_label\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"post_count\"),\n",
    "        F.avg(\"avg_sentiment_score\").alias(\"avg_sentiment\"),\n",
    "        F.avg(\"score\").alias(\"avg_score\"),\n",
    "        F.avg(\"total_comments\").alias(\"avg_comments\"),\n",
    "        F.sum(F.when(F.col(\"is_controversial\"), 1).otherwise(0)).alias(\"controversial_count\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Windowed analytics started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/10 23:09:57 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6e4ede63-0de0-431f-b396-92b3494fcd34. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/12/10 23:09:57 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/10 23:09:57 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write windowed results to memory\n",
    "windowed_query = windowed_sentiment \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"sentiment_trends\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"✓ Windowed analytics started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------+----------+-------------+---------+-------------------+\n",
      "|start              |end                |sentiment_label|post_count|avg_sentiment|avg_score|controversial_count|\n",
      "+-------------------+-------------------+---------------+----------+-------------+---------+-------------------+\n",
      "|2025-12-10 23:05:00|2025-12-10 23:15:00|neutral        |15        |0.036        |125.0    |0                  |\n",
      "|2025-12-10 23:05:00|2025-12-10 23:15:00|positive       |14        |0.181        |159.9    |0                  |\n",
      "|2025-12-10 23:00:00|2025-12-10 23:10:00|neutral        |15        |0.036        |125.0    |0                  |\n",
      "|2025-12-10 23:00:00|2025-12-10 23:10:00|positive       |14        |0.181        |159.9    |0                  |\n",
      "|2025-12-10 22:45:00|2025-12-10 22:55:00|neutral        |16        |0.044        |113.8    |0                  |\n",
      "|2025-12-10 22:45:00|2025-12-10 22:55:00|positive       |12        |0.195        |84.8     |0                  |\n",
      "|2025-12-10 22:40:00|2025-12-10 22:50:00|neutral        |18        |0.043        |109.2    |0                  |\n",
      "|2025-12-10 22:40:00|2025-12-10 22:50:00|positive       |16        |0.195        |153.1    |0                  |\n",
      "|2025-12-10 22:35:00|2025-12-10 22:45:00|positive       |4         |0.197        |358.0    |0                  |\n",
      "|2025-12-10 22:35:00|2025-12-10 22:45:00|neutral        |2         |0.037        |72.0     |0                  |\n",
      "+-------------------+-------------------+---------------+----------+-------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query windowed results\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        window.start,\n",
    "        window.end,\n",
    "        sentiment_label,\n",
    "        post_count,\n",
    "        ROUND(avg_sentiment, 3) as avg_sentiment,\n",
    "        ROUND(avg_score, 1) as avg_score,\n",
    "        controversial_count\n",
    "    FROM sentiment_trends\n",
    "    ORDER BY window.start DESC, post_count DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Read from MongoDB - Analytics on Stored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records in MongoDB: 92\n",
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- analyzed_comments: integer (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- avg_sentiment_score: double (nullable = true)\n",
      " |-- engagement_ratio: double (nullable = true)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- is_controversial: boolean (nullable = true)\n",
      " |-- post_id: integer (nullable = true)\n",
      " |-- sample_comments: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- score: integer (nullable = true)\n",
      " |-- sentiment_label: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- total_comments: integer (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data back from MongoDB\n",
    "mongo_df = spark.read \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .option(\"database\", MONGO_DATABASE) \\\n",
    "    .option(\"collection\", MONGO_COLLECTION) \\\n",
    "    .load()\n",
    "\n",
    "print(f\"Records in MongoDB: {mongo_df.count()}\")\n",
    "mongo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+---------------+-------------------+-----+--------------+\n",
      "|                                                       title|sentiment_label|avg_sentiment_score|score|total_comments|\n",
      "+------------------------------------------------------------+---------------+-------------------+-----+--------------+\n",
      "|                                        Frank Gehry has died|       positive|             0.1013|   46|            10|\n",
      "|                              Netflix to Acquire Warner Bros|        neutral|              0.098| 1310|           168|\n",
      "|                       Cloudflare outage on December 5, 2025|        neutral|             0.0689|  472|            70|\n",
      "|                     Gemini 3 Pro: the frontier of vision AI|       positive|             0.1513|  258|            27|\n",
      "|A $20 drug in Europe requires a prescription and $800 in ...|        neutral|             0.0894|   86|            17|\n",
      "|                Idempotency Keys for Exactly-Once Processing|        neutral|              0.032|   50|             6|\n",
      "|                                            Fizz Buzz in CSS|       positive|             0.1492|   36|             3|\n",
      "|                  Patterns for Defensive Programming in Rust|       positive|             0.1604|  154|            13|\n",
      "|I'm Peter Roberts, immigration attorney who does work for...|        neutral|             0.0845|  147|            58|\n",
      "|                 Most technical problems are people problems|        neutral|             0.0668|  277|            49|\n",
      "+------------------------------------------------------------+---------------+-------------------+-----+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample data from MongoDB\n",
    "mongo_df.select(\n",
    "    \"title\",\n",
    "    \"sentiment_label\",\n",
    "    \"avg_sentiment_score\",\n",
    "    \"score\",\n",
    "    \"total_comments\"\n",
    ").show(10, truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-------------+-----------+\n",
      "|sentiment_label|count|avg_sentiment|avg_upvotes|\n",
      "+---------------+-----+-------------+-----------+\n",
      "|       positive|   47|        0.174|      152.5|\n",
      "|        neutral|   45|        0.045|      151.3|\n",
      "+---------------+-----+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment distribution from MongoDB\n",
    "mongo_df.groupBy(\"sentiment_label\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"count\"),\n",
    "        F.round(F.avg(\"avg_sentiment_score\"), 3).alias(\"avg_sentiment\"),\n",
    "        F.round(F.avg(\"score\"), 1).alias(\"avg_upvotes\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. MongoDB Aggregation Queries with Spark\n",
    "\n",
    "Demonstrate MongoDB's aggregation pipeline pushdown capabilities with Spark.\n",
    "\n",
    "This section shows how Spark can leverage MongoDB's aggregation framework to:\n",
    "- Push down complex queries to MongoDB (reducing data transfer)\n",
    "- Perform server-side filtering and aggregation\n",
    "- Utilize MongoDB's powerful aggregation operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Simple Filter Query - Find Positive Posts\n",
    "\n",
    "Use MongoDB's `$match` operator to filter documents before loading into Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Top 10 most positive posts (unique titles):\n",
      "+-------------------+-----+---------------+------------------------------------------------------------+--------------+\n",
      "|avg_sentiment_score|score|sentiment_label|                                                       title|total_comments|\n",
      "+-------------------+-----+---------------+------------------------------------------------------------+--------------+\n",
      "|             0.3027|  132|       positive|I got an Nvidia GH200 server for €7.5k on Reddit and conv...|            17|\n",
      "|             0.2838| 1101|       positive|                                                Size of Life|            77|\n",
      "|             0.2484|  114|       positive|Gundam is just the same as Jane Austen but happens to inc...|            20|\n",
      "|             0.2362|  149|       positive|Synadia and TigerBeetle Pledge $512k to the Zig Software ...|            12|\n",
      "|             0.2345|   91|       positive|                                  Factor 0.101 now available|             7|\n",
      "|               0.23|   55|       positive|How Google Maps allocates survival across London's restau...|             7|\n",
      "|             0.2084|    6|       positive|                                  Should CSS be constraints?|            12|\n",
      "|             0.2059|   44|       positive|             Scientists create ultra fast memory using light|             6|\n",
      "|             0.1912|   62|       positive|EFF Launches Age Verification Hub as Resource Against Mis...|             6|\n",
      "|             0.1909|   79|       positive|Show HN: HCB Mobile – financial app built by 17 y/o, proc...|            10|\n",
      "+-------------------+-----+---------------+------------------------------------------------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Filter positive sentiment posts using MongoDB aggregation pipeline\n",
    "# This demonstrates query pushdown - filtering happens in MongoDB, not Spark\n",
    "\n",
    "mongo_db = \"hackernews_analytics\"\n",
    "mongo_coll = \"post_sentiment\"\n",
    "\n",
    "# Define aggregation pipeline as a list of dictionaries\n",
    "positive_pipeline = [\n",
    "    {\n",
    "        # STAGE 1: Match only positive sentiment posts\n",
    "        \"$match\": {\n",
    "            \"sentiment_label\": \"positive\",\n",
    "            \"avg_sentiment_score\": {\"$gt\": 0.15}  # Very positive posts\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # STAGE 2: Sort by sentiment score descending (before grouping)\n",
    "        \"$sort\": {\"avg_sentiment_score\": -1}\n",
    "    },\n",
    "    {\n",
    "        # STAGE 3: Group by title to get unique titles\n",
    "        # Keep the document with highest sentiment score for each title\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$title\",\n",
    "            \"title\": {\"$first\": \"$title\"},\n",
    "            \"sentiment_label\": {\"$first\": \"$sentiment_label\"},\n",
    "            \"avg_sentiment_score\": {\"$first\": \"$avg_sentiment_score\"},\n",
    "            \"score\": {\"$first\": \"$score\"},\n",
    "            \"total_comments\": {\"$first\": \"$total_comments\"}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # STAGE 4: Project only needed fields (remove _id from group)\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"title\": 1,\n",
    "            \"sentiment_label\": 1,\n",
    "            \"avg_sentiment_score\": 1,\n",
    "            \"score\": 1,\n",
    "            \"total_comments\": 1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # STAGE 5: Sort again by sentiment score descending\n",
    "        \"$sort\": {\"avg_sentiment_score\": -1}\n",
    "    },\n",
    "    {\n",
    "        # STAGE 6: Limit to top 10\n",
    "        \"$limit\": 10\n",
    "    }\n",
    "]\n",
    "\n",
    "# Read from MongoDB with aggregation pipeline\n",
    "positive_df = (spark.read.format(\"mongodb\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MONGO_URI)\n",
    "    .option(\"database\", mongo_db)\n",
    "    .option(\"collection\", mongo_coll)\n",
    "    .option(\"aggregation.pipeline\", positive_pipeline)  # Pipeline as list\n",
    "    .load())\n",
    "\n",
    "print(\"✓ Top 10 most positive posts (unique titles):\")\n",
    "positive_df.show(10, truncate=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Engagement Analysis\n",
    "\n",
    "Analyze engagement patterns across different sentiment categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Engagement analysis by sentiment and score buckets:\n",
      "+------------+-------------------+-----------+----------+---------------+---------------+\n",
      "|avg_comments|avg_sentiment_score|avg_upvotes|post_count|score_bucket   |sentiment_label|\n",
      "+------------+-------------------+-----------+----------+---------------+---------------+\n",
      "|63.8        |0.064              |468.0      |10        |High (200+)    |neutral        |\n",
      "|6.6         |0.034              |22.0       |17        |Low (0-50)     |neutral        |\n",
      "|12.7        |0.044              |97.6       |18        |Medium (50-200)|neutral        |\n",
      "|42.8        |0.182              |467.1      |9         |High (200+)    |positive       |\n",
      "|6.3         |0.165              |28.7       |9         |Low (0-50)     |positive       |\n",
      "|16.2        |0.174              |93.3       |29        |Medium (50-200)|positive       |\n",
      "+------------+-------------------+-----------+----------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Engagement analysis by sentiment and score buckets\n",
    "engagement_pipeline = [\n",
    "    {\n",
    "        # STAGE 1: Filter valid posts\n",
    "        \"$match\": {\n",
    "            \"sentiment_label\": {\"$in\": [\"positive\", \"neutral\"]},\n",
    "            \"score\": {\"$gt\": 0}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # STAGE 2: Add computed fields\n",
    "        \"$addFields\": {\n",
    "            \"score_bucket\": {\n",
    "                \"$switch\": {\n",
    "                    \"branches\": [\n",
    "                        {\"case\": {\"$lt\": [\"$score\", 50]}, \"then\": \"Low (0-50)\"},\n",
    "                        {\"case\": {\"$lt\": [\"$score\", 200]}, \"then\": \"Medium (50-200)\"},\n",
    "                        {\"case\": {\"$gte\": [\"$score\", 200]}, \"then\": \"High (200+)\"}\n",
    "                    ],\n",
    "                    \"default\": \"Unknown\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # STAGE 3: Group by sentiment and score bucket\n",
    "        \"$group\": {\n",
    "            \"_id\": {\n",
    "                \"sentiment\": \"$sentiment_label\",\n",
    "                \"bucket\": \"$score_bucket\"\n",
    "            },\n",
    "            \"post_count\": {\"$sum\": 1},\n",
    "            \"avg_sentiment_score\": {\"$avg\": \"$avg_sentiment_score\"},\n",
    "            \"avg_comments\": {\"$avg\": \"$total_comments\"},\n",
    "            \"avg_upvotes\": {\"$avg\": \"$score\"}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # STAGE 4: Sort\n",
    "        \"$sort\": {\"_id.sentiment\": 1, \"_id.bucket\": 1}\n",
    "    },\n",
    "    {\n",
    "        # STAGE 5: Reshape\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"sentiment_label\": \"$_id.sentiment\",\n",
    "            \"score_bucket\": \"$_id.bucket\",\n",
    "            \"post_count\": 1,\n",
    "            \"avg_sentiment_score\": {\"$round\": [\"$avg_sentiment_score\", 3]},\n",
    "            \"avg_comments\": {\"$round\": [\"$avg_comments\", 1]},\n",
    "            \"avg_upvotes\": {\"$round\": [\"$avg_upvotes\", 1]}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "engagement_df = (spark.read.format(\"mongodb\")\n",
    "    .option(\"spark.mongodb.read.connection.uri\", MONGO_URI)\n",
    "    .option(\"database\", mongo_db)\n",
    "    .option(\"collection\", mongo_coll)\n",
    "    .option(\"aggregation.pipeline\", engagement_pipeline)\n",
    "    .load())\n",
    "\n",
    "print(\"✓ Engagement analysis by sentiment and score buckets:\")\n",
    "engagement_df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active query: None - Status: {'message': 'Waiting for next trigger', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "Active query: hn_sentiment_live - Status: {'message': 'Getting offsets from KafkaV2[Subscribe[hackernews_posts]]', 'isDataAvailable': False, 'isTriggerActive': True}\n",
      "Active query: sentiment_trends - Status: {'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "# Check active streaming queries\n",
    "for query in spark.streams.active:\n",
    "    print(f\"Active query: {query.name} - Status: {query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping query: None\n",
      "Stopping query: hn_sentiment_live\n",
      "Stopping query: sentiment_trends\n",
      "All streaming queries stopped.\n"
     ]
    }
   ],
   "source": [
    "# Stop all streaming queries\n",
    "for query in spark.streams.active:\n",
    "    print(f\"Stopping query: {query.name}\")\n",
    "    query.stop()\n",
    "\n",
    "print(\"All streaming queries stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
